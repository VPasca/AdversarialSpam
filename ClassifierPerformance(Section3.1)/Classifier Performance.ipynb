{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages needed for navigating file paths\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\vladp\\OneDrive\\Desktop\\VLAD\\Education\\UCL\\Year 3\\Dissertation\\Attack data\\predictions cnn for confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists that will store labels (predicted and original labels in the test Trec07p dataset)\n",
    "CNNBatchLabels = []\n",
    "CNNPredLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7637\n"
     ]
    }
   ],
   "source": [
    "#this file has the prediction labels from cnn model\n",
    "with open(\"cnn epoch 5 prediction labels.txt\", \"r\", errors='ignore') as file:\n",
    "    TrecLines = file.readlines()\n",
    "    \n",
    "    for line in TrecLines:\n",
    "        \n",
    "        x = line.split()\n",
    "        for i in x:\n",
    "            CNNPredLabels.append(i)\n",
    "        \n",
    "\n",
    "print(len(CNNPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7637\n"
     ]
    }
   ],
   "source": [
    "#the file being read has the original labels\n",
    "with open(\"epoch five batch labels.txt\", \"r\", errors='ignore') as file:\n",
    "    TrecLines = file.readlines()\n",
    "    \n",
    "    for line in TrecLines:\n",
    "        \n",
    "        x = line.split()\n",
    "        for i in x:\n",
    "            CNNBatchLabels.append(i)\n",
    "            \n",
    "print(len(CNNBatchLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '0', '0']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(CNNBatchLabels[:5])\n",
    "print(type(CNNBatchLabels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5133\n",
      "2504\n"
     ]
    }
   ],
   "source": [
    "# apologies for the confusing names but I realised countHam was acutally counting spam and countSpam counting ham\n",
    "# the cell below explains this \n",
    "countHam = 0 \n",
    "countSpam = 0\n",
    "for i in CNNBatchLabels:\n",
    "    if i == '0':\n",
    "        countHam+= 1\n",
    "    else: countSpam+=1\n",
    "        \n",
    "print(countHam) \n",
    "print(countSpam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What i realised at this point is that I think 0 was actually spam and 1 was ham. the reason I say this is because in the trec test data set there should be 5107 spam and 2481 ham. Note, there were 49 with no label in the dataset, so it means that 26 non label ones were classified as spam and 23 non label ones were marked as ham. this is essentially my fault when I wrote these labels to the files but it is easily fixable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['REAL', 'REAL', 'FAKE', 'REAL', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "print(CNNPredLabels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sit A 5111\n",
      "Sit B 71\n",
      "Sit C 22\n",
      "Sit D 2433\n"
     ]
    }
   ],
   "source": [
    "SitA = 0\n",
    "SitB = 0\n",
    "SitC = 0\n",
    "SitD = 0\n",
    "\n",
    "index = 0\n",
    "for i in CNNBatchLabels:\n",
    "    \n",
    "    if (i == \"0\") and (CNNPredLabels[index] == \"REAL\"): \n",
    "        SitA +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "    if (i == \"0\") and (CNNPredLabels[index] == \"FAKE\"): \n",
    "        SitC +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "    if (i == \"1\") and (CNNPredLabels[index] == \"REAL\"): \n",
    "        SitB +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "    if (i == \"1\") and (CNNPredLabels[index] == \"FAKE\"): \n",
    "        SitD +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "print(\"Sit A \" + str(SitA)) # although non-intuitive this is actually the true positive (spam classified)\n",
    "print(\"Sit B \" + str(SitB)) # this is actually false positive (misclassify ham as spam)\n",
    "print(\"Sit C \" + str(SitC)) # this is actually false negative (misclassify spam as ham)\n",
    "print(\"Sit D \" + str(SitD)) # this is actually true negative (ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.78224433678146\n"
     ]
    }
   ],
   "source": [
    "#calculating the accuracy for cnn model\n",
    "Accuracy = (SitD + SitA)/(SitA + SitB + SitC + SitD)\n",
    "print((Accuracy)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.62987263604785\n",
      "99.10386965376782\n"
     ]
    }
   ],
   "source": [
    "#calculating precision for cnn model\n",
    "PrecisionSpam = 5111/(5111 + 71)\n",
    "PrecisionHam = 2433/(2433 + 22)\n",
    "print(PrecisionSpam*100)\n",
    "print(PrecisionHam*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.57140074030781\n",
      "97.16453674121406\n"
     ]
    }
   ],
   "source": [
    "#calculating recall for cnn model\n",
    "RecallSpam = 5111/(5111 + 22)\n",
    "RecallHam = 2433/(2433 + 71)\n",
    "print(RecallSpam*100)\n",
    "print(RecallHam*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.09840038778476\n",
      "98.12462189957654\n"
     ]
    }
   ],
   "source": [
    "# calculating f1 for cnn model\n",
    "F1Spam = 2*((PrecisionSpam*RecallSpam)/(PrecisionSpam + RecallSpam))\n",
    "F1Ham = 2*((PrecisionHam*RecallHam)/(PrecisionHam + RecallHam))\n",
    "print(F1Spam*100)\n",
    "print(F1Ham*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\vladp\\OneDrive\\Desktop\\VLAD\\Education\\UCL\\Year 3\\Dissertation\\Attack data\\predictions cnn for confusion matrix\\lstm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will store the labels for lstm (predicted and original labels)\n",
    "LSTMBatchLabels = []\n",
    "LSTMPredLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7637\n"
     ]
    }
   ],
   "source": [
    "with open(\"lstm batch labels.txt\", \"r\", errors='ignore') as file:\n",
    "    TrecLines = file.readlines()\n",
    "    \n",
    "    for line in TrecLines:\n",
    "        \n",
    "        x = line.split()\n",
    "        for i in x:\n",
    "            LSTMBatchLabels.append(i)\n",
    "        \n",
    "\n",
    "print(len(LSTMBatchLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "print(LSTMBatchLabels[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2504\n",
      "5133\n"
     ]
    }
   ],
   "source": [
    "# here the variable names countHam and countSpam correctly reflect what label they are storing now (after discovering\n",
    "# the problem for the cnn)\n",
    "countHam = 0 \n",
    "countSpam = 0\n",
    "for i in LSTMBatchLabels:\n",
    "    if i == '0':\n",
    "        countSpam+= 1\n",
    "    else: countHam+=1\n",
    "        \n",
    "print(countHam)\n",
    "print(countSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7637\n"
     ]
    }
   ],
   "source": [
    "#this file contains the predicted lstm labels\n",
    "with open(\"lstm pred labels.txt\", \"r\", errors='ignore') as file:\n",
    "    TrecLines = file.readlines()\n",
    "    \n",
    "    for line in TrecLines:\n",
    "        \n",
    "        x = line.split()\n",
    "        for i in x:\n",
    "            LSTMPredLabels.append(i)\n",
    "        \n",
    "\n",
    "print(len(LSTMPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True pos 5121\n",
      "True neg 2497\n",
      "False pos 7\n",
      "False neg 12\n"
     ]
    }
   ],
   "source": [
    "# again here it was my fault in how I recorded the labels and predictions to my file \n",
    "# so I had to rearrange what was tp, tn, fp, fn.\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "index = 0\n",
    "for i in LSTMBatchLabels:\n",
    "    \n",
    "    if (i == \"0\") and (LSTMPredLabels[index] == \"REAL\"): \n",
    "        TP +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "    if (i == \"0\") and (LSTMPredLabels[index] == \"FAKE\"): \n",
    "        FN +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "    if (i == \"1\") and (LSTMPredLabels[index] == \"REAL\"): \n",
    "        FP +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "    if (i == \"1\") and (LSTMPredLabels[index] == \"FAKE\"): \n",
    "        TN +=1\n",
    "        index +=1\n",
    "        continue\n",
    "        \n",
    "print(\"True pos \" + str(TP))\n",
    "print(\"True neg \" + str(TN))\n",
    "print(\"False pos \" + str(FP))\n",
    "print(\"False neg \" + str(FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.75121120858977\n"
     ]
    }
   ],
   "source": [
    "# accuracy of lstm\n",
    "AccuracyLSTM = (TP + TN)/(TP+TN+FN+FP)\n",
    "print(AccuracyLSTM*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.8634945397816\n",
      "99.52172180151454\n"
     ]
    }
   ],
   "source": [
    "#precison of lstm\n",
    "LSTMPrSpam = TP/(TP+FP)\n",
    "LSTMPrHam = TN/(TN+FN)\n",
    "print(LSTMPrSpam*100)\n",
    "print(LSTMPrHam*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.76621858562244\n",
      "99.72044728434504\n"
     ]
    }
   ],
   "source": [
    "# recall of lstm\n",
    "LSTMRecSp = TP/(TP+FN)\n",
    "LSTMRecHa = TN/(TN+FP)\n",
    "print(LSTMRecSp*100) \n",
    "print(LSTMRecHa*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.81483286229414\n",
      "99.62098543786156\n"
     ]
    }
   ],
   "source": [
    "# f1 for lstm \n",
    "LSTMf1Spam = 2*((LSTMPrSpam*LSTMRecSp)/(LSTMPrSpam+LSTMRecSp))\n",
    "LSTMf1Ham = 2*((LSTMPrHam*LSTMRecHa)/(LSTMPrHam+LSTMRecHa))\n",
    "print(LSTMf1Spam*100) \n",
    "print(LSTMf1Ham*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
